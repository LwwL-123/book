# GMP 并发调度器深度解析

> 并发（并行），一直以来都是一个编程语言里的核心主题之一，也是被开发者关注最多的话题；Go 语言作为一个出道以来就自带 『高并发』光环的富二代编程语言，它的并发（并行）编程肯定是值得开发者去探究的，而 Go 语言中的并发（并行）编程是经由 goroutine 实现的，goroutine 是 golang 最重要的特性之一，具有使用成本低、消耗资源低、能效高等特点，官方宣称原生 goroutine 并发成千上万不成问题，于是它也成为 Gopher 们经常使用的特性。



# Goroutine & Scheduler

**Goroutine**，Go 语言基于并发（并行）编程给出的自家的解决方案。goroutine 是什么？通常 goroutine 会被当做 coroutine（协程）的 golang 实现，从比较粗浅的层面来看，这种认知也算是合理，但实际上，goroutine 并非传统意义上的协程，现在主流的线程模型分三种：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），传统的协程库属于**用户级线程模型**，而 goroutine 和它的 `Go Scheduler` 在底层实现上其实是属于**两级线程模型**，因此，有时候为了方便理解可以简单把 goroutine 类比成协程，但心里一定要有个清晰的认知 — goroutine 并不等同于协程。



## 线程那些事儿

互联网时代以降，由于在线用户数量的爆炸，单台服务器处理的连接也水涨船高，迫使编程模式由从前的串行模式升级到并发模型，而几十年来，并发模型也是一代代地升级，有 IO 多路复用、多进程以及多线程，这几种模型都各有长短，现代复杂的高并发架构大多是几种模型协同使用，不同场景应用不同模型，扬长避短，发挥服务器的最大性能，而多线程，因为其轻量和易用，成为并发编程中使用频率最高的并发模型，而后衍生的协程等其他子产品，也都基于它，而我们今天要分析的 goroutine 也是基于线程，因此，我们先来聊聊线程的三大模型：

线程的实现模型主要有 3 种：内核级线程模型、用户级线程模型和两级线程模型（也称混合型线程模型），它们之间最大的差异就在于用户线程与内核调度实体（KSE，Kernel Scheduling Entity）之间的对应关系上。而所谓的内核调度实体 KSE 就是指可以被操作系统内核调度器调度的对象实体（这说的啥玩意儿，敢不敢通俗易懂一点？）。简单来说 KSE 就是**内核级线程**，是操作系统内核的最小调度单元，也就是我们写代码的时候通俗理解上的线程了（这么说不就懂了嘛！装什么 13）。



### 用户级线程模型

用户线程与内核线程 KSE 是多对一（N : 1）的映射模型，多个用户线程的一般从属于单个进程并且多线程的调度是由用户自己的线程库来完成，线程的创建、销毁以及多线程之间的协调等操作都是==由用户自己的线程库来负责而无须借助系统调用来实现==。一个进程中所有创建的线程都只和同一个 KSE 在运行时动态绑定，也就是说，操作系统只知道用户进程而对其中的线程是无感知的，==内核的所有调度都是基于用户进程==。许多语言实现的 **协程库** 基本上都属于这种方式（比如 python 的 gevent）。由于线程调度是在用户层面完成的，也就是相较于内核调度不需要让 CPU 在用户态和内核态之间切换，这种实现方式相比内核级线程可以做的很轻量级，对系统资源的消耗会小很多，因此可以创建的线程数量与上下文切换所花费的代价也会小得多。



但该模型有个原罪：并不能做到真正意义上的并发，假设在某个用户进程上的某个用户线程因为一个阻塞调用（比如 I/O 阻塞）而被 CPU 给中断（抢占式调度）了，那么该进程内的所有线程都被阻塞（因为单个用户进程内的线程自调度是没有 CPU 时钟中断的，从而没有轮转调度），整个进程被挂起。即便是多 CPU 的机器，也无济于事，因为在用户级线程模型下，一个 CPU 关联运行的是整个用户进程，进程内的子线程绑定到 CPU 执行是由用户进程调度的，内部线程对 CPU 是不可见的，此时可以理解为 CPU 的调度单位是用户进程。所以很多的**协程库**会把自己一些阻塞的操作重新封装为完全的非阻塞形式，然后在以前要阻塞的点上，主动让出自己，并通过某种方式通知或唤醒其他待执行的用户线程在该 KSE 上运行，从而避免了内核调度器由于 KSE 阻塞而做上下文切换，这样整个进程也不会被阻塞了。



### 内核级线程模型

用户线程与内核线程 KSE 是一对一（1 : 1）的映射模型，也就是每一个用户线程绑定一个实际的内核线程，而线程的调度则完全交付给操作系统内核去做，应用程序对线程的创建、终止以及同步都基于内核提供的系统调用来完成，大部分编程语言的线程库(比如 Java 的 java.lang.Thread、C++11 的 std::thread 等等)都是对操作系统的线程（内核级线程）的一层封装，创建出来的每个线程与一个独立的 KSE 静态绑定，因此其调度完全由操作系统内核调度器去做，也就是说，一个进程里创建出来的多个线程每一个都绑定一个 KSE。这种模型的优势和劣势同样明显：优势是实现简单，直接借助操作系统内核的线程以及调度器，所以 CPU 可以快速切换调度线程，于是多个线程可以同时运行，因此相较于用户级线程模型它真正做到了并行处理；但它的劣势是，由于直接借助了操作系统内核来创建、销毁和以及多个线程之间的上下文切换和调度，因此资源成本大幅上涨，且对性能影响很大。



### 两级线程模型

两级线程模型是博采众长之后的产物，充分吸收前两种线程模型的优点且尽量规避它们的缺点。在此模型下，用户线程与内核 KSE 是多对多（N : M）的映射模型：首先，区别于用户级线程模型，两级线程模型中的一个进程可以与多个内核线程 KSE 关联，也就是说一个进程内的多个线程可以分别绑定一个自己的 KSE，这点和内核级线程模型相似；其次，又区别于内核级线程模型，它的进程里的线程并不与 KSE 唯一绑定，而是可以多个用户线程映射到同一个 KSE，当某个 KSE 因为其绑定的线程的阻塞操作被内核调度出 CPU 时，其关联的进程中其余用户线程可以重新与其他 KSE 绑定运行。所以，两级线程模型既不是用户级线程模型那种完全靠自己调度的也不是内核级线程模型完全靠操作系统调度的，而是中间态（自身调度与系统调度协同工作），也就是 — 『薛定谔的模型』（误），因为这种模型的高度复杂性，操作系统内核开发者一般不会使用，所以更多时候是作为第三方库的形式出现，而 Go 语言中的 runtime 调度器就是采用的这种实现方案，实现了 Goroutine 与 KSE 之间的动态关联，不过 Go 语言的实现更加高级和优雅；该模型为何被称为两级？**即用户调度器实现用户线程到 KSE 的『调度』，内核调度器实现 KSE 到 CPU 上的『调度』**。



## G-P-M 模型概述

每一个 OS 线程都有一个固定大小的内存块(一般会是 2MB)来做栈，这个栈会用来存储当前正在被调用或挂起(指在调用其它函数时)的函数的内部变量。这个固定大小的栈同时很大又很小。因为 2MB 的栈对于一个小小的 goroutine 来说是很大的内存浪费，而对于一些复杂的任务（如深度嵌套的递归）来说又显得太小。因此，Go 语言做了它自己的『线程』。



在 Go 语言中，每一个 goroutine 是一个独立的执行单元，相较于每个 OS 线程固定分配 2M 内存的模式，goroutine 的栈采取了动态扩容方式， 初始时仅为 2KB，随着任务执行按需增长，最大可达 1GB（64 位机器最大是 1G，32 位机器最大是 256M），且完全由 golang 自己的调度器 **Go Scheduler** 来调度。此外，GC 还会周期性地将不再使用的内存回收，收缩栈空间。 因此，Go 程序可以同时并发成千上万个 goroutine 是得益于它强劲的调度器和高效的内存模型。Go 的创造者大概对 goroutine 的定位就是屠龙刀，因为他们不仅让 goroutine 作为 golang 并发编程的最核心组件（开发者的程序都是基于 goroutine 运行的）而且 golang 中的许多标准库的实现也到处能见到 goroutine 的身影，比如 net/http 这个包，甚至语言本身的组件 runtime 运行时和 GC 垃圾回收器都是运行在 goroutine 上的，作者对 goroutine 的厚望可见一斑。



任何用户线程最终肯定都是要交由 OS 线程来执行的，goroutine（称为 G）也不例外，但是 G 并不直接绑定 OS 线程运行，而是由 Goroutine Scheduler 中的 P - *Logical Processor* （逻辑处理器）来作为两者的『中介』，P 可以看作是一个抽象的资源或者一个上下文，一个 P 绑定一个 OS 线程，在 golang 的实现里把 OS 线程抽象成一个数据结构：M，G 实际上是由 M 通过 P 来进行调度运行的，但是在 G 的层面来看，P 提供了 G 运行所需的一切资源和环境，因此在 G 看来 P 就是运行它的 “CPU”，由 G、P、M 这三种由 Go 抽象出来的实现，最终形成了 Go 调度器的基本结构：

 

- G: 表示 Goroutine，每个 Goroutine 对应一个 G 结构体，G 存储 Goroutine 的运行堆栈、状态以及任务函数，可重用。G 并非执行体，每个 G 需要绑定到 P 才能被调度执行。
- P: Processor，表示逻辑处理器， 对 G 来说，P 相当于 CPU 核，G 只有绑定到 P(在 P 的 local runq 中)才能被调度。对 M 来说，P 提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等，P 的数量决定了系统内最大可并行的 G 的数量（前提：物理 CPU 核数 >= P 的数量），P 的数量由用户设置的 GOMAXPROCS 决定，但是不论 GOMAXPROCS 设置为多大，P 的数量最大为 256。
- M: Machine，OS 线程抽象，代表着真正执行计算的资源，在绑定有效的 P 后，进入 schedule 循环；而 schedule 循环的机制大致是从 Global 队列、P 的 Local 队列以及 wait 队列中获取 G，切换到 G 的执行栈上并执行 G 的函数，调用 goexit 做清理工作并回到 M，如此反复。M 并不保留 G 状态，这是 G 可以跨 M 调度的基础，M 的数量是不定的，由 Go Runtime 调整，为了防止创建过多 OS 线程导致系统调度不过来，目前默认最大限制为 10000 个。

关于 P，我们需要再絮叨几句，在 Go 1.0 发布的时候，它的调度器其实 G-M 模型，也就是没有 P 的，调度过程全由 G 和 M 完成，这个模型暴露出一些问题：

- 单一全局互斥锁(Sched.Lock)和集中状态存储的存在导致所有 goroutine 相关操作，比如：创建、重新调度等都要上锁；
- goroutine 传递问题：M 经常在 M 之间传递『可运行』的 goroutine，这导致调度延迟增大以及额外的性能损耗；
- 每个 M 做内存缓存，导致内存占用过高，数据局部性较差；
- 由于 syscall 调用而形成的剧烈的 worker thread 阻塞和解除阻塞，导致额外的性能损耗。



这些问题实在太扎眼了，导致 Go1.0 虽然号称原生支持并发，却在并发性能上一直饱受诟病，然后，Go 语言委员会中一个核心开发大佬看不下了，亲自下场重新设计和实现了 Go 调度器（在原有的 G-M 模型中引入了 P）并且实现了一个叫做 [*work-stealing*](https://supertech.csail.mit.edu/papers/steal.pdf) 的调度算法：

- 每个 P 维护一个 G 的本地队列；
- 当一个 G 被创建出来，或者变为可执行状态时，就把他放到 P 的可执行队列中；
- 当一个 G 在 M 里执行结束后，P 会从队列中把该 G 取出；如果此时 P 的队列为空，即没有其他 G 可以执行， M 就随机选择另外一个 P，从其可执行的 G 队列中取走一半。

该算法避免了在 goroutine 调度时使用全局锁。

![img](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325172715.png)

## G-P-M 模型调度

Go 调度器工作时会维护两种用来保存 G 的任务队列：一种是一个 Global 任务队列，一种是每个 P 维护的 Local 任务队列。

当通过 `go` 关键字创建一个新的 goroutine 的时候，它会优先被放入 P 的本地队列。为了运行 goroutine，M 需要持有（绑定）一个 P，接着 M 会启动一个 OS 线程，循环从 P 的本地队列里取出一个 goroutine 并执行。当然还有上文提及的 `work-stealing` 调度算法：当 M 执行完了当前 P 的 Local 队列里的所有 G 后，P 也不会就这么在那躺尸啥都不干，它会先尝试从 Global 队列寻找 G 来执行，如果 Global 队列为空，它会随机挑选另外一个 P，从它的队列里中拿走一半的 G 到自己的队列中执行。



**如果一切正常，调度器会以上述的那种方式顺畅地运行，但这个世界没这么美好，总有意外发生，以下分析 goroutine 在两种例外情况下的行为。**

Go runtime 会在下面的 goroutine 被阻塞的情况下运行另外一个 goroutine：

- blocking syscall (for example opening a file)
- network input
- channel operations
- primitives in the sync package

这四种场景又可归类为两种类型：

### 用户态阻塞/唤醒

当 goroutine 因为 channel 操作或者 network I/O 而阻塞时（实际上 golang 已经用 netpoller 实现了 goroutine 网络 I/O 阻塞不会导致 M 被阻塞，仅阻塞 G，这里仅仅是举个栗子），对应的 G 会被放置到某个 wait 队列(如 channel 的 waitq)，该 G 的状态由 `_Gruning` 变为 `_Gwaitting` ，而 M 会跳过该 G 尝试获取并执行下一个 G，如果此时没有 runnable 的 G 供 M 运行，那么 M 将解绑 P，并进入 sleep 状态；当阻塞的 G 被另一端的 G2 唤醒时（比如 channel 的可读/写通知），G 被标记为 runnable，尝试加入 G2 所在 P 的 runnext，然后再是 P 的 Local 队列和 Global 队列。

### 系统调用阻塞

当 G 被阻塞在某个系统调用上时，此时 G 会阻塞在 `_Gsyscall` 状态，M 也处于 block on syscall 状态，此时的 M 可被抢占调度：执行该 G 的 M 会与 P 解绑，而 P 则尝试与其它 idle 的 M 绑定，继续执行其它 G。如果没有其它 idle 的 M，但 P 的 Local 队列中仍然有 G 需要执行，则创建一个新的 M；当系统调用完成后，G 会重新尝试获取一个 idle 的 P 进入它的 Local 队列恢复执行，如果没有 idle 的 P，G 会被标记为 runnable 加入到 Global 队列。

 



# 协程让出、抢占、监控和调度

## 协程主动让出

- timer

协程在执行time.sleep时，状态会从Grunning变为Gwaiting，并进入到对应timer中等待，而timer中持有一个回调函数，在指定时间到达后，调用这个回调函数，把等待的协程恢复到Grunnable状态，并放回到runq中。那么谁负责在指定时间到达后，调用这个回调函数呢？

其实每个P中都有一个最小堆，存储在p.timers中，用于管理自己的timer，堆顶timer就是接下来要触发的那一个，每次调度时，都会调用checktimers函数，检查并执行那些已经到时间的timer。不过这不够稳妥，如果所有m都在忙，不能及时触发调度的话，就可能导致timer执行时间发生较大的差距，所以还会通过监控线程来增加一些保障。

监控线程是由main.goroutine创建的，这个监控线程和GMP中的工作线程不同，并不需要依赖P，也不由GPM模型调度，他会重复执行一系列任务，视情况来调整自己的休眠时间，其中一项就是保障timer正常执行，他会在空不出m时，创建新的工作线程，以保障timer可以正常执行

![image-20220321182813354](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325172723.png)

- channel

在等待channel时，协程也会从Grunning变为Gwaiting，同时会进入channel的对应读队列或者写队列，如果协程需要等待IO时间，就也需要让出，以epoll为例，若IO事件尚未就绪，需要注册要等待的IO事件到监听队列中，而每个监听对象都可以关联一个event data，所以就在这里记录是哪个协程在等待，等到时间就绪时再把它恢复到runq中即可。

而获取就绪的IO事件，需要主动轮询，所以为了降低IO的延迟，需要时不时的轮询以下，也就是执行netpoll。实际上，监控线程、调度器、GC等过程中都会按需执行netpoll

![image-20220321183715203](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325172727.png)



## 抢占式调度

我们知道GC开始前要STW来进行开启写屏障等准备工作，所以STW就是要抢占所有的P，

而这段程序中，在双核cpu中，在go1.13以前，居然会死锁，也就是程序中有协程没被抢占，一直在执行，而STW一直在等待他让出

```go
func main() {
    go func() {
      i := 0
      for {
        println(i)
        i++
      }
    }
  
    for {

    }
}
```



### Go1.13 基于栈增长

为什么：

STW要抢占所有的P，所以他会记录下自己要等待多少个P让出，当这个值减为0，目的就达到了。对于当前P，陷入系统调用的P，空闲状态的P，直接设置为GCstop状态即可。对于还有G正在运行的P，会将对应的g.stackguard0设置为一个特殊标识，告诉他GC正在等待让出，此外还会设置一个gcwaiting标识。

在goroutine创建的时候，栈的大小是固定2k的，为了防止出现栈溢出的情况，编译器会在有明显栈消耗的函数头部插入一些监测代码，但如果g.stackguard0被设置为特殊标识，就不会执行栈增长，而是会执行一次调度，在调度执行时，会检测gcwaiting标识，若发现gc在等待，则会让出当前P，设置为gcstop状态，也就是说，上面的代码空的for循环，没有执行函数，也就没有机会执行栈增长代码，也就不会开始调度了，所以他不知道gc在等待他让出。



### Go1.14 异步抢占

在1.13中，依赖栈增长检测代码的抢占方式，遇到没有函数调用，就会出现问题。

在1.14中，实现了基于信号的异步抢占方式。

首先如果g.stackguard0被设置为特殊标识，首先会判断硬件是否支持异步抢占，和用户是否允许开启异步抢占，如果都通过了就会吧p.preempt设置为true，并且交由preemptM来执行异步抢占，他的主要逻辑是通过runtime.signalM来向指定M发送sigPreempt信号，也就是通过操作系统中信号相关的系统调用，将指定信号发送给目标线程。



工作线程接收到信号以后，会调用对应信号的handler来处理，确定信号为sigPreempt信号以后，会首先确认runtime是否对指定的G进行异步抢占,也就是判断g.preempt和p.preempt是否为true，并且G还要在Grunning状态，然后还要确认在当前位置打断G是安全的，怎么确认呢，首先指定的G可以挂起并安全的扫描他的栈和寄存器，并且当前被打断的位置并没有打断写屏障，第二，指定的G还有足够的栈空间来注入一个异步函数调用，第三，可以安全的和runtime进行交互，主要是确认没有持有runtime相关的锁，继而不会再后面获得锁时造成死锁。

确认了抢占是安全的以后，就可以放心的通过pushcall，向G的上下文注入异步抢占函数调用了，处理完信号后，被中断的G得到恢复，立刻执行被注入的异步抢占函数，该函数会执行runtime中的调度逻辑，实现让出



### 抢占系统调用的P

为了充分利用CPU，还会抢占处理系统调用中的P，因为一个协程如果要系统调用，就要切换到g0栈，也就是说在系统调用结束之前，m和g就绑定了，这时P就空闲了。这时就会接触m和p的关联，只在m.oldp中记录这个P。这时P会关联到其他m，继续工作。如果g和m从系统调用中恢复，会先检查之前的P是否被占用，没有的话继续使用，否则就会在申请一个P，没有申请到的话，就会把当前G放到全局runq中去，然后当前线程睡眠



## 调度

不管是抢占还是让出后，m都不能闲着，这就是schedule的职责了。

schedule要给这个M找到一个G，首先会判断M和G是否绑定，如果绑定就不能执行其他G，阻塞当前M，当G再次得到调度执行时，会自己吧M唤醒。

如果没有绑定，就先看看GC是不是在等待执行，全局变量sched有一个gcwaiting标识，如果gc在等待执行，就去执行gc，回来在执行调度程序，后检查有没有要执行的timer，然后有1/61的几率去全局runq中获取一部分g，到本地runq，获取下一个待执行的g会先从本地runq中查找，没有的话从全局runq获取一部分，在没有的话，会执行netpolll，恢复那些io时间已经就绪的G，会被放回到全局runq中，最后会尝试从其他p窃取一部分任务，得到G后，会绑定g和m，恢复协程执行现场，继续执行。

