#  Raft 分布式共识算法

原文：*https://www.youtube.com/watch?v=vYp4LYbnnW8*

## 目标

Raft 的目标（或者说是分布式共识算法的目标）是：**保证 log 完全相同地复制到多台服务器上**。

![image-20220105140648487](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174353.png)

只要每台服务器的日志相同，那么，在不同服务器上的状态机以相同顺序从日志中执行相同的命令，将会产生相同的结果。

共识算法的工作就是管理这些日志。



## 系统模型

我们假设：

- 服务器可能会宕机、会停止运行过段时间再恢复，但是**非拜占庭的**（即它的行为是非恶意的，不会篡改数据等）；
- 网络通信会中断，消息可能会丢失、延迟或乱序；可能会网络分区；

Raft 是基于 Leader 的共识算法，故主要考虑：

- Leader 正常运行
- Leader 故障，必须选出新的 Leader



我们将从 6 个部分解释 Raft：

1. Leader 选举；
2. 正常运行：日志复制（最简单的部分）；
3. Leader 变更时的安全性和一致性（最棘手、最关键的部分）；
4. 处理旧 Leader：旧的 Leader 并没有真的下线怎么办？
5. 客户端交互：实现线性化语义(linearizable semantics)；
6. 配置变更：如何在集群中增加或删除节点；



## 开始之前

开始之前需要了解 Raft 的一些术语。

### 服务器状态

服务器在任意时间只能处于以下三种状态之一：

- Leader：处理所有客户端请求、日志复制。同一时刻最多只能有一个可行的 Leader；
- Follower：完全被动的（不发送 RPC，只响应收到的 RPC）——大多数服务器在大多数情况下处于此状态；
- Candidate：用来选举新的 Leader，处于 Leader 和 Follower 之间的暂时状态；

**系统正常运行时，只有一个 Leader，其余都是 Followers.**

状态转换图：

![image-20220105140719628](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174357.png)



### 任期

时间被划分成一个个的**任期(Term)**，每个任期都由一个数字来表示任期号，任期号单调递增并且永远不会重复。

![image-20220105140725648](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174359.png)

一个正常的任期至少有一个 Leader，通常分为两部分：

- 任期开始时的选举过程；
- 正常运行的部分；

有些任期可能没有选出 Leader（如图 Term 3），这时候会立即进入下一个任期，再次尝试选出一个 Leader。

每个节点维护一个 `currentTerm` 变量，表示系统中当前任期。`currentTerm` **必须持久化存储**，以便在服务器宕机重启时将其恢复。

**任期非常重要！任期能够帮助 Raft 识别过期的信息。**例如：如果 `currentTerm = 2` 的节点与 `currentTerm = 3` 的节点通信，我们可以知道第一个节点上的信息是过时的。

我们只使用最新任期的信息。后面我们会遇到各种情况，去检测和消除不是最新任期的信息。

### 两个 RPC

Raft 中服务器之间所有类型的通信通过两个 RPC 调用：

- `RequestVote`：用于选举；
- `AppendEntries`：用于复制 log 和发送心跳；



## 1. Leader 选举

![image-20220105140732120](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174403.png)

- 节点启动时，都是 Follower 状态；

- Follower 被动地接受 Leader 或 Candidate 的 RPC；

- 所以，如果 Leader 想要保持权威，必须向集群中的其它节点发送心跳包（空的 `AppendEntries RPC`）；

- 等待选举超时(`electionTimeout`，一般在 100~500ms)后，Follower 没有收到任何 RPC：

- - Follower 认为集群中没有 Leader
  - 开始新的一轮选举



### 选举

当一个节点开始竞选：

- 增加自己的 `currentTerm`
- 转为 Candidate 状态，**其目标是获取超过半数节点的选票，让自己成为 Leader**
- **先给自己投一票**
- 并行地向集群中其它节点发送 `RequestVote RPC` 索要选票，如果没有收到指定节点的响应，它会反复尝试，直到发生以下三种情况之一：

1. 获得超过半数的选票：成为 Leader，并向其它节点发送 `AppendEntries` 心跳；
2. 收到来自 Leader 的 RPC：转为 Follower；
3. 其它两种情况都没发生，没人能够获胜(`electionTimeout` 已过)：增加 `currentTerm`，开始新一轮选举；



流程图如下：

![image-20220105140737797](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174407.png)

### 选举安全性

选举过程需要保证两个特性：**安全性(safety)**和**活性(liveness)**。



安全性(safety)：一个任期内只会有一个 Leader 被选举出来。需要保证：

- 每个节点在同一任期内只能投一次票，它将投给第一个满足条件的投票请求，然后拒绝其它 Candidate 的请求。这需要持久化存储投票信息 `votedFor`，以便宕机重启后恢复，否则重启后 `votedFor` 丢失会导致投给别的节点；
- 只有获得超过半数节点的选票才能成为 Leader，也就是说，两个不同的 Candidate 无法在同一任期内都获得超过半数的票；



活性(liveness)：确保最终能选出一个 Leader。

问题是：原则上我们可以无限重复分割选票，假如选举同一时间开始，同一时间超时，同一时间再次选举，如此循环。

解决办法很简单：

- 节点随机选择超时时间，通常在 [T, 2T] 之间（T = `electionTimeout`）
- 这样，节点不太可能再同时开始竞选，先竞选的节点有足够的时间来索要其他节点的选票
- T >> broadcast time(T 远大于广播时间)时效果更佳



## 2. 日志复制

### 日志结构

![image-20220105140743908](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174411.png)

每个节点存储自己的日志副本(`log[]`)，每条日志记录包含：

- 索引：该记录在日志中的位置
- 任期号：该记录首次被创建时的任期号
- 命令



**日志必须持久化存储。**一个节点必须先将记录安全写到磁盘，才能向系统中其他节点返回响应。

如果一条日志记录被存储在超过半数的节点上，我们认为该记录**已提交**(`committed`)——这是 Raft 非常重要的特性！如果一条记录已提交，意味着状态机可以安全地执行该记录。

在上图中，第 1-7 条记录被提交，第 8 条尚未提交。

> 提醒：多数派复制了日志即已提交，这个定义并不精确，我们会在后面稍作修改。



### 正常运行

- 客户端向 Leader 发送命令，希望该命令被所有状态机执行；

- Leader 先将该命令追加到自己的日志中；

- Leader 并行地向其它节点发送 `AppendEntries RPC`，等待响应；

- 收到超过半数节点的响应，则认为新的日志记录是被提交的：

- - Leader 将命令传给自己的状态机，然后向客户端返回响应
  - 此外，一旦 Leader 知道一条记录被提交了，将在后续的 `AppendEntries RPC` 中通知已经提交记录的 Followers
  - Follower 将已提交的命令传给自己的状态机

- 如果 Follower 宕机/超时：Leader 将反复尝试发送 RPC；

- 性能优化：Leader 不必等待每个 Follower 做出响应，只需要超过半数的成功响应（确保日志记录已经存储在超过半数的节点上）——一个很慢的节点不会使系统变慢，因为 Leader 不必等他；



### 日志一致性

Raft 尝试在集群中保持日志较高的一致性。

**Raft 日志的 index 和 term 唯一标示一条日志记录。**（这非常重要！！！）

1. 如果两个节点的日志在相同的索引位置上的任期号相同，则认为他们具有一样的命令；**从头到这个索引位置之间的日志完全相同**；
2. **如果给定的记录已提交，那么所有前面的记录也已提交**。

### `AppendEntries` 一致性检查

Raft 通过 `AppendEntries RPC` 来检测这两个属性。

- 对于每个 `AppendEntries RPC` 包含新日志记录**之前那条记录的**索引(`prevLogIndex`)和任期(`prevLogTerm`)；
- Follower 检查自己的 index 和 term 是否与 `prevLogIndex` 和 `prevLogTerm` 匹配，匹配则接收该记录；否则拒绝；

![image-20220105140752355](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174415.png)



## 3. Leader 更替

当新的 Leader 上任后，日志可能不会非常干净，因为前一任领导可能在完成日志复制之前就宕机了。**Raft 对此的处理方式是：无需采取任何特殊处理。**

当新 Leader 上任后，他不会立即进行任何清理操作，他将会在正常运行期间进行清理。

原因是当一个新的 Leader 上任时，往往意味着有机器故障了，那些机器可能宕机或网络不通，所以没有办法立即清理他们的日志。在机器恢复运行之前，我们必须保证系统正常运行。

**大前提是 Raft 假设了 Leader 的日志始终是对的。**所以 Leader 要做的是，随着时间推移，让所有 Follower 的日志最终都与其匹配。

但与此同时，Leader 也可能在完成这项工作之前故障，日志会在一段时间内堆积起来，从而造成看起来相当混乱的情况，如下所示：

![image-20220105140757702](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174418.png)

因为我们已经知道 index 和 term 是日志记录的唯一标识符，这里不再显示日志包含的命令，下同。

如图，这种情况可能出现在 S4 和 S5 是任期 2、3、4 的 Leader，但不知何故，他们没有复制自己的日志记录就崩溃了，系统分区了一段时间，S1、S2、S3 轮流成为了任期 5、6、7 的 Leader，但无法与 S4、S5 通信以进行日志清理——所以我们看到的日志非常混乱。

**唯一重要的是，索引 1-3 之间的记录是已提交的(已存在多数派节点)，因此我们必须确保留下它们**。

其它日志都是未提交的，我们还没有将这些命令传递给状态机，也没有客户端会收到这些执行的结果，所以不管是保留还是丢弃它们都无关紧要。



### 安全性

**一旦状态机执行了一条日志里的命令，必须确保其它状态机在同样索引的位置不会执行不同的命令。**

Raft 安全性(Safety)：如果某条日志记录在某个任期号已提交，那么这条记录必然出现在更大任期号的未来 Leader 的日志中。



这保证了安全性要求：

- Leader 不会覆盖日志中的记录；
- 只有 Leader 的日志中的记录才能被提交；
- 在应用到状态机之前，日志必须先被提交；



这决定我们要修改选举程序：

- 如果节点的日志中没有正确的内容，需要避免其成为 Leader；
- 稍微修改 committed 的定义（*即前面提到的要稍作修改*）：前面说多数派存储即是已提交的，但在某些时候，我们必须延迟提交日志记录，直到我们知道这条记录是安全的，**所谓安全的，就是我们认为后续 Leader 也会有这条日志**。



### 延迟提交，选出最佳 Leader

问题来了：我们如何确保选出了一个很好地保存了所有已提交日志的 Leader ？

这有点棘手，举个例子：假设我们要在下面的集群中选出一个新 Leader，但此时第三台服务器不可用。

![image-20220105140802981](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174423.png)

这种情况下，仅看前两个节点的日志我们无法确认是否达成多数派，故无法确认第五条日志是否已提交。

那怎么办呢？

通过比较日志，在选举期间，选择最有可能包含所有已提交的日志：

- Candidate 在 `RequestVote RPCs` 中包含日志信息（最后一条记录的 index 和 term，记为 `lastIndex` 和 `lastTerm`）；
- 收到此投票请求的服务器 V 将比较谁的日志更完整：`(lastTermV > lastTermC) ||(lastTermV == lastTermC) && (lastIndexV > lastIndexC)` 将拒绝投票；（即：V 的任期比 C 的任期新，或任期相同但 V 的日志比 C 的日志更完整）；
- 无论谁赢得选举，可以确保 Leader 和超过半数投票给它的节点中拥有最完整的日志——**最完整的意思就是 index 和 term 这对唯一标识是最大的**。



### 举个例子

#### Case 1: Leader 决定提交日志

![image-20220105140808214](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174426.png)

任期 2 的 Leader S1 的 index = 4 日志刚刚被复制到 S3，并且 Leader 可以看到 index = 4 已复制到超过半数的服务器，那么该日志可以提交，并且安全地应用到状态机。

现在，这条记录是安全的，下一任期的 Leader 必须包含此记录，因此 S4 和 S5 都不可能从其它节点那里获得选票：S5 任期太旧，S4 日志太短。

只有前三台中的一台可以成为新的 Leader——S1 当然可以，S2、S3 也可以通过获取 S4 和 S5 的选票成为 Leader。



#### Case 2: Leader 试图提交之前任期的日志

![image-20220105140813989](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174429.png)

如图所示的情况，在任期 2 时记录仅写在 S1 和 S2 两个节点上，由于某种原因，任期 3 的 Leader S5 并不知道这些记录，S5 创建了自己的三条记录然后宕机了，然后任期 4 的 Leader S1 被选出，S1 试图与其它服务器的日志进行匹配。因此它复制了任期 2 的日志到 S3。

**此时 index=3 的记录时是不安全的**。

因为 S1 可能在此时宕机，然后 S5 可能从 S2、S3、S4 获得选票成为任期 5 的 Leader。一旦 S5 成为新 Leader，它将覆盖 index=3-5 的日志，S1-S3 的这些记录都将消失。

我们还要需要一条新的规则，来处理这种情况。

### 新的 Commit 规则

新的选举不足以保证日志安全，我们还需要继续修改 commit 规则。

Leader 要提交一条日志：

- 日志必须存储在超过半数的节点上；
- **Leader 必须看到：超过半数的节点上还必须存储着至少一条自己任期内的日志**；

![image-20220105140825042](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174436.png)

如图，回到上面的 Case 2: 当 index = 3 & term = 2 被复制到 S3 时，它还不能提交该记录，必须等到 term = 4 的记录存储在超过半数的节点上，此时 index = 3 和 index = 4 可以认为是已提交。

此时 S5 无法赢得选举了，它无法从 S1-S3 获得选票。

**结合新的选举规则和 commit 规则，我们可以保证 Raft 的安全性。**



### 日志不一致

![image-20220105140830656](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174440.png)

Leader 变更可能导致日志的不一致，这里展示一种可能的情况。

可以从图中看出，Raft 集群中通常有两种不一致的日志：

- 缺失的记录(Missing Entries)；
- 多出来的记录(Extraneous Entries)；

我们要做的就是清理这两种日志。

### 修复 Follower 日志

新的 Leader 必须使 Follower 的日志与自己的日志保持一致，通过：

- 删除 Extraneous Entries；
- 补齐 Missing Entries；

Leader 为每个 Follower 保存 `nextIndex`：

- 下一个要发送给 Follower 的日志索引；
- 初始化为：1 + Leader 最后一条日志的索引；

Leader 通过 `nextIndex` 来修复日志。当 `AppendEntries RPC` 一致性检查失败，递减 `nextIndex` 并重试。如下图所示：

![image-20220105140835680](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174443.png)

对于 a：

- 一开始 `nextIndex` = 11，带上日志 index = 10 & term = 6，检查失败；
- `nextIndex` = 10，带上日志 index = 9 & term = 6，检查失败；
- 如此反复，直到 `nextIndex` = 5，带上日志 index = 4 & term = 4，该日志现在匹配，会在 a 中补齐 Leader 的日志。如此往下补齐。

对于 b：
会一直检查到 `nextIndex` = 4 才匹配。值得注意的是，对于 b 这种情况，当 Follower 覆盖不一致的日志时，它将删除所有后续的日志记录（任何无关紧要的记录之后的记录也都是无关紧要的）。如下图所示：

![image-20220105140841305](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174446.png)

## 4. 处理旧 Leader

实际上，老的 Leader 可能不会马上消失，例如：网络分区将 Leader 与集群的其余部分分隔，其余部分选举出了一个新的 Leader。问题在于，如果老的 Leader 重新连接，也不知道新的 Leader 已经被选出来，它会尝试作为 Leader 继续提交日志。此时如果有客户端向老 Leader 发送请求，老的 Leader 会尝试存储该命令并向其它节点复制日志——我们必须阻止这种情况发生。

**任期就是用来发现过时的 Leader**(和 Candidates)：

- 每个 RPC 都包含发送方的任期；
- 如果发送方的任期太老，无论哪个过程，RPC 都会被拒绝，发送方转变到 Follower 并更新其任期；
- 如果接收方的任期太老，接收方将转为 Follower，更新它的任期，然后正常的处理 RPC；

由于新 Leader 的选举会更新超过半数服务器的任期，旧的 Leader 不能提交新的日志，因为它会联系至少一台多数派集群的节点，然后发现自己任期太老，会转为 Follower 继续工作。

这里不打算继续讨论别的极端情况。



## 5. 客户端协议

客户端只将命令发送到 Leader：

- 如果客户端不知道 Leader 是谁，它会和任意一台服务器通信；
- 如果通信的节点不是 Leader，它会告诉客户端 Leader 是谁；

Leader 直到将命令记录、提交和执行到状态机之前，不会做出响应。

这里的问题是如果 Leader 宕机会导致请求超时：

- 客户端重新发出命令到其他服务器上，最终重定向到新的 Leader
- 用新的 Leader 重试请求，直到命令被执行

这留下了一个命令可能被执行两次的风险——Leader 可能在执行命令之后但响应客户端之前宕机，此时客户端再去寻找下一个 Leader，同一个命令就会被执行两次——这是不可接受的！

解决办法是：客户端发送给 Leader 的每个命令都带上一个唯一 id

- Leader 将唯一 id 写到日志记录中
- 在 Leader 接受命令之前，先检查其日志中是否已经具有该 id
- 如果 id 在日志中，说明是重复的请求，则忽略新的命令，返回旧命令的响应

**每个命令只会被执行一次，这就是所谓的线性化的关键要素**。

# **二、 pbft算法**

因为网上已经有大量文章对raft算法进行过详细的介绍，因此这部分只会简单的阐述算法的基本原理和流程。raft 算法包含三种角色，分别是：跟随者（ follower ），候选人（candidate ）和领导者（ leader ）。集群中的一个节点在某一时刻只能是这三种状态的其中一种，这三种角色是可以随着时间和条件的变化而互相转换的。

raft 算法主要有两个过程：一个过程是领导者选举，另一个过程是日志复制，其中日志复制过程会分记录日志和提交数据两个阶段。raft 算法支持最大的容错故障节点是（N-1）/2，其中 N 为 集群中总的节点数量。



pbft 算法的提出主要是为了解决拜占庭将军问题。什么是拜占庭将军问题呢？在已知有将军是叛徒的情况下，其余忠诚的将军如何达成一致协议的问题，这就是拜占庭将军问题。



### **1.raft和pbft的最大容错节点数**

**首先我们先来思考一个问题，为什么 pbft 算法的最大容错节点数量是（n-1）/3，而 raft 算法的最大容错节点数量是（n-1）/2 ？**

对于raft算法，raft算法的的容错只支持容错故障节点，不支持容错作恶节点。什么是故障节点呢？就是节点因为系统繁忙、宕机或者网络问题等其它异常情况导致的无响应，出现这种情况的节点就是故障节点。那什么是作恶节点呢？作恶节点除了可以故意对集群的其它节点的请求无响应之外，还可以故意发送错误的数据，或者给不同的其它节点发送不同的数据，使整个集群的节点最终无法达成共识，这种节点就是作恶节点。

raft 算法只支持容错故障节点，假设集群总节点数为n，故障节点为 f ，根据小数服从多数的原则，集群里正常节点只需要比 f 个节点再多一个节点，即 f+1 个节点，正确节点的数量就会比故障节点数量多，那么集群就能达成共识。因此 raft 算法支持的最大容错节点数量是（n-1）/2。

对于 pbft 算法，因为 pbft 算法的除了需要支持容错故障节点之外，还需要支持容错作恶节点。假设集群节点数为 N，有问题的节点为 f。有问题的节点中，可以既是故障节点，也可以是作恶节点，或者只是故障节点或者只是作恶节点。那么会产生以下两种极端情况：



1. 第一种情况，f 个有问题节点既是故障节点，又是作恶节点，那么根据小数服从多数的原则，集群里正常节点只需要比f个节点再多一个节点，即 f+1 个节点，确节点的数量就会比故障节点数量多，那么集群就能达成共识。也就是说这种情况支持的最大容错节点数量是 （n-1）/2。
2. 第二种情况，故障节点和作恶节点都是不同的节点。那么就会有 f 个问题节点和 f 个故障节点，当发现节点是问题节点后，会被集群排除在外，剩下 f 个故障节点，那么根据小数服从多数的原则，集群里正常节点只需要比f个节点再多一个节点，即 f+1 个节点，确节点的数量就会比故障节点数量多，那么集群就能达成共识。所以，所有类型的节点数量加起来就是 f+1 个正确节点，f个故障节点和f个问题节点，即 3f+1=n。

结合上述两种情况，因此 pbft 算法支持的最大容错节点数量是（n-1）/3。





### **2.算法基本流程**

pbft 算法的基本流程主要有以下四步：

1. 客户端发送请求给主节点 
2. 主节点广播请求给其它节点，节点执行 pbft 算法的三阶段共识流程。
3. 节点处理完三阶段流程后，返回消息给客户端。
4. 客户端收到来自 f+1 个节点的相同消息后，代表共识已经正确完成。

为什么收到 f+1 个节点的相同消息后就代表共识已经正确完成？从上一小节的推导里可知，无论是最好的情况还是最坏的情况，如果客户端收到 f+1 个节点的相同消息，那么就代表有足够多的正确节点已全部达成共识并处理完毕了。



### **3.算法核心三阶段流程**

下面介绍 pbft 算法的核心三阶段流程，如下图所示：

![img](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174452.jpg)

算法的核心三个阶段分别是 pre-prepare 阶段（预准备阶段），prepare 阶段（准备阶段）， commit 阶段（提交阶段）。图中的C代表客户端，0，1，2，3 代表节点的编号，打叉的3代表可能是故障节点或者是问题节点，这里表现的行为就是对其它节点的请求无响应。0 是主节点。整个过程大致是如下：



首先，客户端向主节点发起请求，主节点 0 收到客户端请求，会向其它节点发送 pre-prepare 消息，其它节点就收到了pre-prepare 消息，就开始了这个核心三阶段共识过程了。

1. Pre-prepare 阶段：节点收到 pre-prepare 消息后，会有两种选择，一种是接受，一种是不接受。什么时候才不接受主节点发来的 pre-prepare 消息呢？一种典型的情况就是如果一个节点接受到了一条 pre-pre 消息，消息里的 v 和 n 在之前收到里的消息是曾经出现过的，但是 d 和 m 却和之前的消息不一致，或者请求编号不在高低水位之间（高低水位的概念在下文会进行解释），这时候就会拒绝请求。拒绝的逻辑就是主节点不会发送两条具有相同的 v 和 n ，但 d 和 m 却不同的消息。

2. Prepare 阶段：节点同意请求后会向其它节点发送 prepare 消息。这里要注意一点，同一时刻不是只有一个节点在进行这个过程，可能有 n 个节点也在进行这个过程。因此节点是有可能收到其它节点发送的 prepare 消息的。在一定时间范围内，如果收到超过 2f 个不同节点的 prepare 消息，就代表 prepare 阶段已经完成。

3. Commit 阶段：于是进入 commit 阶段。向其它节点广播 commit 消息，同理，这个过程可能是有 n 个节点也在进行的。因此可能会收到其它节点发过来的 commit 消息，当收到 2f+1 个 commit 消息后（包括自己），代表大多数节点已经进入 commit 阶段，这一阶段已经达成共识，于是节点就会执行请求，写入数据。



处理完毕后，节点会返回消息给客户端，这就是 pbft 算法的全部流程。为了更清晰的展现 这个过程和一些细节，下面以流程图来表示这个过程：

![img](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220325174455.jpg)

```
注解：
V：当前视图的编号。视图的编号是什么意思呢？比如当前主节点为 A，视图编号为 1，如果主节点换成 B，那么视图编号就为 2，这个概念和 raft 的 term 任期是很类似的。
N：当前请求的编号。主节点收到客户端的每个请求都以一个编号来标记。
M：消息的内容
d或D（m）：消息内容的摘要
i： 节点的编号
```



