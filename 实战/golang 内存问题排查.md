# golang 内存问题排查

## 如何解决

线上出现问题，首先要解决问题，不影响用户使用，后在寻找根因，给出方法

1. 排查项目故障期间，是否有项目发布，如果有，尝试回滚

2. 排查项目故障期间，是否有配置变更，如果有，尝试回滚配置

3. 排查近期项目的代码变更：包括代码、配置是否有变更，如果有，组织review



除此之外，如果问题没有解决

1、排查基础平台&工具侧是否有升级或变更

2、排查依赖方是否有变更（）

3、**重点检查**运营侧是否有管理平台的变更导致影响到了线上业务



## 排查过程

1. 是否存在goroutine逃逸？

查看故障前，goroutine是否有升高现象。

2. 是否代码出现了内存泄露？

通过pprof或coredump的内存快照，进行内存泄漏排查。

3. 其他问题导致的OOM？



## 几个概念

内存溢出 OOM （out of memory），是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个int,但给它存了long才能存下的数，那就是内存溢出。

内存泄露（ memory leak），是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光。最终的结果就是导致OOM。

### 1. goroutine逃逸

1. 通常内存泄露的主因就是 goroutine 过多, goroutine 申请过多，增长速度快于释放速度，就会导致 goroutine 越来越多。
2. I/O 问题，I/O 连接未设置超时时间，导致 goroutine 一直在等待。如在请求第三方网络连接接口时，因网络问题一直没有接到返回结果，如果没有设置超时时间，则代码会一直阻塞。
3. 互斥锁未释放，goroutine 无法获取到锁资源，导致 goroutine 阻塞。假设有一个共享变量，goroutineA 对共享变量加锁但未释放，导致其他 goroutineB、goroutineC、...、goroutineN 都无法获取到锁资源，导致其他 goroutine 发生阻塞。
4. waitgroup 使用不当，waitgroup 的 Add、Done 和 wait 数量不匹配，会导致 wait 一直在等待。
5. select阻塞，使用 select 但 case 未覆盖全面，导致没有 case 就绪，最终 goroutine 阻塞。
6. channel 阻塞

- 写阻塞

- - 无缓冲 channel 的阻塞通常是写操作因为没有读而阻塞
  - 有缓冲的 channel 因为缓冲区满了，写操作阻塞

- 读阻塞

- - 期待从 channel 读数据，结果没有 goroutine 往进写

7. slice 引起内存泄露，两个 slice 共享地址，其中一个为全局变量，另一个也无法被 gc；append slice 后一直使用，未进行清理。

```go
var a []int
func test(b []int) {
        a = b[:3]
        return
}
```

 所以排查问题时，首先看近期的goroutine数量是否增多，什么原因导致gouroutine过多，是否是请求囤积，如何解决



### 2. 代码内存泄漏

#### 2.1 为什么要避免内存泄漏

因为如果变量的内存发生逃逸，它的生命周期就是不可知的，其会被分配到堆上，而堆上分配内存不能像栈一样会自动释放，为了解放程序员双手，专注于业务的实现，go实现了gc垃圾回收机制，但gc会影响程序运行性能，所以要尽量减少程序的gc操作。

#### 2.2 引发内存逃逸的常见情况

1. 在方法内把局部变量指针返回，被外部引用，其生命周期大于栈，则溢出。
2. 发送指针或带有指针的值到channel，因为编译时候无法知道那个goroutine会在channel接受数据，编译器无法知道什么时候释放。
3. 在一个切片上存储指针或带指针的值。比如[]*string，导致切片内容逃逸，其引用值一直在堆上。
4. 因为切片的append导致超出容量，切片重新分配地址，切片背后的存储基于运行时的数据进行扩充，就会在堆上分配。
5. 在interface类型上调用方法，在Interface调用方法是动态调度的，只有在运行时才知道。

#### 2.3 如何避免内存逃逸

1. 不要盲目使用变量指针作为参数，虽然减少了复制，但变量逃逸的开销更大。
2. 预先设定好slice长度，避免频繁超出容量，重新分配。
3. 一个经验是，指针指向的数据大部分在堆上分配的，请注意。





## 线上问题oom

- 收到大批SLB报警、服务依赖告警

- 发现resource-service服务双机房全部节点均有重启，排查STD日志分析重启原因

- **重启原因是服务OOM**

- 发现**无配置变更和发版操作**

- 陆续有更多依赖resource-service的服务反馈受到影响，如AI侧、播放页、【我的】页等。

- 发现天马服务QPS上涨了1k，原因暂不明（后续排查主要是因为SLB失败重试导致）

- 尝试在discovery平台使用pprof dump OOM实例的内存快照，但始终抓不到OOM实例的内存快照（后续排查崩溃时间太快，在几秒内，人工难以捕捉）

- resource-service的协程数量暴涨（经后续排查，发现OOM前协程数量稳定，并非导致OOM的原因，监控看到的协程数量尖刺是容器被重新拉起后SLB重试、请求堆积后，大量请求涌入导致的）

- 首先解决问题：尝试将内存配置从**软限4G/硬限8G**增大到**软限8G/硬限12G**，然后重新发布resource-service服务，观察后效果并不理想，仍旧会产生OOM。

- 排查上下游相关服务发版的影响可能性

- 播放页网关服务增加对resource-service服务的兜底降级，避免因resource-service彻底挂掉后造成天马和播放页抖动或无法使用的影响。

- 天马和播放页对resource-service的超时配置不合理，需要减少超时时间配置做快速失败的处理，避免被resource-service拖垮。

- 超时配置为50ms

- 回滚线上8台容器到上一个版本，让新老版本同时运行，用以排除新版本并非引起本次事件的原因。

- 1）resource-service应用集群，所有容器实例大部分在3-5秒内，从内存1.8g占用到OOM迅速崩溃，人工pprof几乎不可能抓到内存快照；

  2）resource-service应用集群，所有容器实例短时间内同时崩溃，基本可以排除一些偶然因素，大概率是代码bug触发，触发条件不明，仍需排查；

- 发现服务中 ugctab 的配置在4.6当天有配置变更，且与问题出现的时间点吻合;曾尝试上传一个较大的配置文件，其中包含了约23w的avid, 总大小约3.1M；

- 最终定位代码，resource-service的ugcTab接口在请求时会构造avidMap，其中会包含23w avid的值。

  由于ugcTab接口是在播放页调用，qps非常高，非常短的时间内就把内存打满导致oom发生；

![image-20220627142405531](https://picture-1258612855.cos.ap-shanghai.myqcloud.com/20220627142405.png)

缓存数据结构设计不合理，且在每次请求时将缓存数据load出来，重新赋值新数据结构，在碰到大缓存数据，高QPS的请求量时，极易发生OOM



平台侧是否增加OOM前自动pprof，这次OOM太快，研发侧人工dump没有找到有效的内存快照，没有复现到底是什么导致的内存堆积